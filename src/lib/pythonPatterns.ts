// Python patterns implementation
export const pythonPatterns = {
  'react': "# ReAct Agent implementation\nimport openai\nimport json\nfrom typing import Dict, List, Any, Optional, Union\n\nclass ReActAgent:\n    def __init__(self, client, model: str = \"gpt-4\"):\n        self.client = client\n        self.model = model\n        \n    async def execute(self, query: str, max_cycles: int = 5) -> Dict[str, Any]:\n        \"\"\"Execute the ReAct agent to solve a problem through reasoning and action cycles.\"\"\"\n        try:\n            current_cycle = 0\n            done = False\n            context_history = []\n            final_answer = ''\n            \n            # Add initial query to context\n            context_history.append(f\"User query: {query}\")\n            \n            # Available tools\n            tools = {\n                'search': (lambda query: {\n                    'results': f\"Search results for '{query}'\",\n                    'links': [\"https://example.com/1\", \"https://example.com/2\"]\n                }),\n                'calculate': (lambda expression: {\n                    'result': eval(expression)\n                }),\n                'weather': (lambda location: {\n                    'temperature': 72,\n                    'condition': 'sunny',\n                    'location': location\n                })\n            }\n            \n            while not done and current_cycle < max_cycles:\n                current_cycle += 1\n                \n                # Step 1: Reasoning phase\n                print(f\"Cycle {current_cycle}: Reasoning...\")\n                \n                reasoning_prompt = f\"\"\"You are a ReAct agent that solves problems through cycles of reasoning and action.\n\nTask: {query}\n\nPrevious interactions:\n{context_history}\n\nFirst, think about how to approach this problem. Then, if needed, select a tool to use:\n- search: Search for information online\n- calculate: Perform mathematical calculations\n- weather: Get weather information for a location\n\nFormat your response as:\nThought: <your reasoning>\nAction: <tool_name>\nAction Input: <input for the tool>\n\nOr if you can provide the final answer:\nThought: <your reasoning>\nFinal Answer: <your answer>\n\"\"\"\n\n                # Simulate LLM call\n                # In a real implementation, this would be an API call to an LLM\n                reasoning_response = \"Thought: I need to calculate the distance between two points (3,4) and (0,0)\\nAction: calculate\\nAction Input: ((3**2 + 4**2) ** 0.5)\"\n                \n                context_history.append(reasoning_response)\n                \n                # Check if the response contains a final answer\n                if \"Final Answer:\" in reasoning_response:\n                    answer_match = reasoning_response.split(\"Final Answer:\")\n                    if len(answer_match) > 1:\n                        final_answer = answer_match[1].strip()\n                        done = True\n                        continue\n                \n                # Extract action and action input if present\n                action_match = reasoning_response.split(\"Action:\")\n                if len(action_match) > 1:\n                    action_and_input = action_match[1].strip().split(\"\\n\")\n                    tool_name = action_and_input[0].strip()\n                    \n                    action_input_match = reasoning_response.split(\"Action Input:\")\n                    if len(action_input_match) > 1:\n                        tool_input = action_input_match[1].strip()\n                        \n                        # Step 2: Action phase - call the appropriate tool\n                        print(f\"Cycle {current_cycle}: Taking action with tool '{tool_name}'...\")\n                        \n                        if tool_name in tools:\n                            try:\n                                tool_result = tools[tool_name](tool_input)\n                                observation = f\"Observation: {json.dumps(tool_result, indent=2)}\"\n                                context_history.append(observation)\n                            except Exception as e:\n                                observation = f\"Observation: Error executing {tool_name} with input {tool_input}. Error: {str(e)}\"\n                                context_history.append(observation)\n                        else:\n                            observation = f\"Observation: Tool {tool_name} not found. Available tools: {', '.join(tools.keys())}\"\n                            context_history.append(observation)\n            \n            # Return the final result\n            return {\n                'status': done === true ? 'success' : 'max_cycles_reached',\n                'cycles': current_cycle,\n                'answer': final_answer !== '' ? final_answer : \"No final answer reached.\",\n                'history': context_history\n            }\n            \n        except Exception as e:\n            return {'status': 'error', 'message': str(e)}\",

  'codeact': "# CodeAct Agent implementation\nimport openai\nimport re\nimport json\nfrom typing import Dict, List, Any, Optional, Union\n\nclass CodeActAgent:\n    def __init__(self, client, model: str = \"gpt-4\"):\n        self.client = client\n        self.model = model\n        \n    async def execute(self, query: str, max_cycles: int = 5) -> Dict[str, Any]:\n        \"\"\"Execute the CodeAct agent to solve problems by writing and executing Python code.\"\"\"\n        try:\n            current_cycle = 0\n            done = False\n            context_history = []\n            final_result = ''\n            \n            # Simulate Python code execution environment\n            def execute_code(code):\n                print(\"Executing Python code (simulated):\")\n                print(code)\n                \n                # This is a simulation - in a real implementation, this would execute Python code\n                # and return the results. For this example, we'll return a simulated result.\n                \n                if 'import' in code:\n                    if 'numpy' in code or 'pandas' in code:\n                        return \"Library imported successfully.\"\n                \n                if 'print(' in code:\n                    print_match = re.search(r'print\\\\(([^)]+)\\\\)', code)\n                    if print_match:\n                        return f\"Output: {print_match.group(1)}\"\n                \n                if 'def ' in code:\n                    return \"Function defined successfully.\"\n                \n                # Default simulated response\n                return \"Code executed. Result: [simulated output based on the provided code]\"\n            \n            # Add the initial query to context\n            context_history.append(f\"User query: {query}\")\n            \n            while not done and current_cycle < max_cycles:\n                current_cycle += 1\n                \n                # Generate agent response\n                agent_prompt = \"\"\"You are a CodeAct agent that solves problems by writing and executing Python code.\n\nTask: \"\"\" + query + \"\"\"\n\nPrevious interactions:\n\"\"\" + \"\\n\\n\".join(context_history) + \"\"\"\n\nBased on the current state, either:\n\n1. Write Python code to make progress, formatted as:\n   Thought: <your reasoning>\n   Code:\n   ```\n   # Your Python code here\n   ```\n\n2. Or provide the final answer if you've solved the problem:\n   Thought: <your reasoning>\n   Final Answer: <your answer>\n\"\"\"\n                \n                # Simulate LLM call\n                # In a real implementation, this would be an API call to an LLM\n                agent_response = \"\"\"Thought: I'll write code to calculate factorial of a number.\nCode:\n```\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n# Test with n=5\nresult = factorial(5)\nprint(result)\n```\"\"\"\n                context_history.append(f\"Agent: {agent_response}\")\n                \n                # Check if the response contains a final answer\n                if 'Final Answer:' in agent_response:\n                    answer_match = re.search(r'Final Answer:(.*?)$', agent_response, re.DOTALL)\n                    if answer_match:\n                        final_result = answer_match.group(1).strip()\n                        done = True\n                    \n                # Check if the response contains code\n                elif '```' in agent_response:\n                    # Extract code block\n                    code_match = re.search(r'```\\s*([\\s\\S]*?)\\s*```', agent_response)\n                    if code_match:\n                        code = code_match.group(1).strip()\n                        \n                        # Execute the code (simulated)\n                        execution_result = execute_code(code)\n                        \n                        # Add the observation to the history\n                        context_history.append(f\"Observation: {execution_result}\")\n            \n            return {\n                'status': done === true ? 'success' : 'max_cycles_reached',\n                'cycles': current_cycle,\n                'result': final_result !== '' ? final_result : 'No final result reached.',\n                'history': context_history\n            }\n            \n        except Exception as e:\n            return {'status': 'failed', 'reason': str(e)}\",

  'self-reflection': "# Self-Reflection implementation\nimport openai\nimport re\nimport json\nfrom typing import Dict, List, Any, Optional, Union\n\nclass SelfReflectionAgent:\n    def __init__(self, client, model: str = \"gpt-4\"):\n        self.client = client\n        self.model = model\n        \n    async def execute(self, query: str, max_revisions: int = 3) -> Dict[str, Any]:\n        \"\"\"Execute the Self-Reflection agent to improve responses through iterative critique.\"\"\"\n        try:\n            revisions = 0\n            reflections = []\n            current_response = \"\"\n            \n            # Initial response generation\n            initial_prompt = f\"\"\"You are an AI assistant tasked with answering the following query:\nQuery: {query}\n\nProvide your best response to the query.\n\"\"\"\n            # Simulate LLM call for initial response\n            current_response = \"The factorial of a number n is the product of all positive integers less than or equal to n. It's often denoted as n!. So 5! = 5 × 4 × 3 × 2 × 1 = 120.\"\n            \n            reflections.append({\n                \"iteration\": revisions,\n                \"response\": current_response,\n                \"reflection\": None\n            })\n            \n            while revisions < max_revisions:\n                revisions += 1\n                \n                # Self-reflection phase\n                reflection_prompt = f\"\"\"You are a critical evaluator examining the following response to the query:\n\nQuery: {query}\n\nResponse:\n{current_response}\n\nCritique this response by considering:\n1. Accuracy - Is all information factually correct?\n2. Completeness - Does it fully address the query?\n3. Clarity - Is it easy to understand?\n4. Conciseness - Is it appropriately detailed without being verbose?\n\nProvide specific suggestions for improvement.\n\"\"\"\n                # Simulate LLM call for reflection\n                if revisions == 1:\n                    reflection = \"The response is accurate but could be more complete. It defines factorial and gives an example for 5!, but doesn't explain how to calculate factorials for edge cases like 0! or mention practical applications or algorithmic implementations.\"\n                elif revisions == 2:\n                    reflection = \"The response now covers edge cases and implementation details but could be more concise and better structured. Consider organizing with subheadings for Definition, Examples, Edge Cases, and Implementation.\"\n                else:\n                    reflection = \"The response is now well-structured, accurate, complete, and clear. No further improvements needed.\"\n                \n                # Improvement phase\n                improvement_prompt = f\"\"\"You are an AI assistant tasked with answering the following query:\nQuery: {query}\n\nYour previous response was:\n{current_response}\n\nA critical evaluation identified these issues:\n{reflection}\n\nProvide an improved response addressing these critiques.\n\"\"\"\n                # Simulate LLM call for improved response\n                if revisions == 1:\n                    current_response = \"The factorial of a number n is the product of all positive integers less than or equal to n, denoted as n!.\\n\\nExamples:\\n- 5! = 5 × 4 × 3 × 2 × 1 = 120\\n- 3! = 3 × 2 × 1 = 6\\n\\nEdge cases:\\n- 0! is defined as 1\\n- 1! = 1\\n\\nFactorials can be calculated recursively or iteratively. A recursive implementation in Python would be:\\n```python\\ndef factorial(n):\\n    if n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n```\"\n                elif revisions == 2:\n                    current_response = \"\"\"# Factorial Calculation\n\n## Definition\nThe factorial of a number n (denoted as n!) is the product of all positive integers less than or equal to n.\n\n## Examples\n- 5! = 5 × 4 × 3 × 2 × 1 = 120\n- 3! = 3 × 2 × 1 = 6\n\n## Edge Cases\n- 0! is defined as 1\n- 1! = 1\n\n## Implementation\nPython recursive implementation:\n```python\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n-1)\n```\n\nFactorials grow very quickly and are used in combinatorics, probability, and mathematical analysis.\"\"\"\n                \n                reflections.append({\n                    \"iteration\": revisions,\n                    \"response\": current_response,\n                    \"reflection\": reflection\n                })\n                \n                # Check if further improvement is needed\n                if \"no further improvements needed\" in reflection.lower():\n                    break\n            \n            return {\n                \"status\": \"success\",\n                \"iterations\": revisions,\n                \"final_response\": current_response,\n                \"reflections\": reflections\n            }\n            \n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\",

  'agentic-rag': "# Agentic RAG implementation\nimport openai\nimport json\nimport re\nfrom typing import Dict, List, Any, Optional, Union\n\nclass AgenticRAGAgent:\n    def __init__(self, client, model: str = \"gpt-4\", \n                 vector_store=None, document_processor=None):\n        self.client = client\n        self.model = model\n        self.vector_store = vector_store  # In a real implementation, this would be a vector DB\n        self.document_processor = document_processor  # For processing and chunking documents\n        \n    async def execute(self, query: str) -> Dict[str, Any]:\n        \"\"\"Execute the Agentic RAG agent to retrieve and synthesize information.\"\"\"\n        try:\n            # Simulated document store for demonstration\n            documents = [\n                {\"id\": \"doc1\", \"content\": \"Azure AI Agents are systems that can understand goals and complete tasks.\"},\n                {\"id\": \"doc2\", \"content\": \"Agent patterns include ReAct, Reflexion, and Plan-and-Execute methodologies.\"},\n                {\"id\": \"doc3\", \"content\": \"The Agent Communication Protocol (ACP) enables interoperability between AI agents.\"},\n                {\"id\": \"doc4\", \"content\": \"Model Context Protocol (MCP) provides standardized interactions with AI models.\"},\n                {\"id\": \"doc5\", \"content\": \"Azure OpenAI Service provides secure, enterprise-ready AI capabilities.\"}\n            ]\n            \n            retrieval_steps = []\n            \n            # Step 1: Query Decomposition\n            decomp_prompt = f\"\"\"Analyze the following query and break it down into search terms that would help retrieve relevant information:\n\nQuery: {query}\n\nOutput a JSON object with:\n1. \"search_terms\": Array of specific search terms to look for\n2. \"required_info\": Key information needed to answer the query\n3. \"query_type\": Classification of query (factual, comparison, explanation, etc.)\n\"\"\"\n            # Simulate LLM call\n            decomposition_result = {\n                \"search_terms\": [\"agent patterns\", \"AI agents\", \"Azure AI\", \"communication protocols\"],\n                \"required_info\": [\"Types of agent patterns\", \"How agents communicate\", \"Azure AI integration\"],\n                \"query_type\": \"explanation\"\n            }\n            \n            retrieval_steps.append({\n                \"step\": \"Query Decomposition\",\n                \"output\": decomposition_result\n            })\n            \n            # Step 2: Intelligent Retrieval\n            retrieved_docs = []\n            for term in decomposition_result[\"search_terms\"]:\n                # In a real implementation, this would use semantic search\n                matching_docs = [doc for doc in documents if term.lower() in doc[\"content\"].lower()]\n                retrieved_docs.extend(matching_docs)\n            \n            # Remove duplicates\n            unique_docs = []\n            for doc in retrieved_docs:\n                if doc not in unique_docs:\n                    unique_docs.append(doc)\n            \n            retrieval_steps.append({\n                \"step\": \"Document Retrieval\",\n                \"output\": unique_docs\n            })\n            \n            # Step 3: Relevance Assessment\n            assessment_prompt = f\"\"\"Assess the relevance of each document to the query:\n\nQuery: {query}\n\nDocuments:\n{json.dumps(unique_docs, indent=2)}\n\nFor each document, rate its relevance from 0-10 and explain why.\n\"\"\"\n            # Simulate LLM call\n            relevance_assessment = [\n                {\"doc_id\": \"doc2\", \"relevance\": 9, \"reason\": \"Directly addresses agent patterns\"},\n                {\"doc_id\": \"doc3\", \"relevance\": 8, \"reason\": \"Discusses agent communication\"},\n                {\"doc_id\": \"doc1\", \"relevance\": 7, \"reason\": \"Provides general context on AI agents\"},\n                {\"doc_id\": \"doc4\", \"relevance\": 6, \"reason\": \"Related to model interaction\"},\n                {\"doc_id\": \"doc5\", \"relevance\": 5, \"reason\": \"Mentions Azure services but less specific to patterns\"}\n            ]\n            \n            # Sort by relevance\n            relevance_assessment.sort(key=lambda x: x[\"relevance\"], reverse=True)\n            \n            retrieval_steps.append({\n                \"step\": \"Relevance Assessment\",\n                \"output\": relevance_assessment\n            })\n            \n            # Step 4: Information Synthesis\n            content_for_synthesis = \"\\n\\n\".join([\n                doc[\"content\"] for doc in unique_docs if \n                any(assessment[\"doc_id\"] == doc[\"id\"] and assessment[\"relevance\"] >= 6\n                    for assessment in relevance_assessment)\n            ])\n            \n            synthesis_prompt = f\"\"\"Synthesize an answer to the query using the following relevant information:\n\nQuery: {query}\n\nRelevant Information:\n{content_for_synthesis}\n\nProvide a comprehensive answer that addresses all aspects of the query.\n\"\"\"\n            # Simulate LLM call\n            synthesis_result = \"Agent patterns are methodologies for designing AI agents with specific capabilities. Key patterns include ReAct (Reasoning and Acting), Reflexion (self-improvement through reflection), and Plan-and-Execute. These agents communicate through protocols like the Agent Communication Protocol (ACP), which enables interoperability between different AI systems. The Model Context Protocol (MCP) standardizes interactions with AI models, creating a consistent interface for agent-model communication.\"\n            \n            retrieval_steps.append({\n                \"step\": \"Information Synthesis\",\n                \"output\": synthesis_result\n            })\n            \n            return {\n                \"status\": \"success\",\n                \"answer\": synthesis_result,\n                \"process\": retrieval_steps,\n                \"sources\": [doc[\"id\"] for doc in unique_docs if \n                          any(assessment[\"doc_id\"] == doc[\"id\"] and assessment[\"relevance\"] >= 6\n                              for assessment in relevance_assessment)]\n            }\n            \n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\",

  'routing': "# Routing Agent pattern implementation\nimport openai\nimport json\nfrom typing import Dict, List, Any, Optional, Union\n\nclass RoutingAgent:\n    def __init__(self, client, model: str = \"gpt-4\"):\n        self.client = client\n        self.model = model\n        \n    async def execute(self, query: str) -> Dict[str, Any]:\n        \"\"\"Route a query to the most appropriate specialized agent.\"\"\"\n        try:\n            # Define specialized agents\n            specialized_agents = {\n                \"math\": {\n                    \"description\": \"Handles mathematical calculations and problems\",\n                    \"capabilities\": [\"algebra\", \"calculus\", \"statistics\", \"probability\"]\n                },\n                \"code\": {\n                    \"description\": \"Writes, reviews, and debugs code\",\n                    \"capabilities\": [\"python\", \"javascript\", \"algorithms\", \"debugging\"]\n                },\n                \"research\": {\n                    \"description\": \"Finds and synthesizes information on topics\",\n                    \"capabilities\": [\"search\", \"summarization\", \"fact-checking\"]\n                },\n                \"creative\": {\n                    \"description\": \"Generates creative content like stories or ideas\",\n                    \"capabilities\": [\"writing\", \"ideation\", \"storytelling\"]\n                }\n            }\n            \n            # Step 1: Analyze the query to determine the best agent\n            routing_prompt = f\"\"\"Analyze the following query and determine which specialized agent should handle it:\n\nQuery: {query}\n\nAvailable agents:\n{json.dumps(specialized_agents, indent=2)}\n\nReturn a JSON object with:\n1. \"selected_agent\": The name of the best agent to handle this query\n2. \"confidence\": Your confidence score (0-1) in this selection\n3. \"reasoning\": Your reasoning for this selection\n4. \"fallback_agent\": Second-best agent if the first is unavailable\n\"\"\"\n            # Simulate LLM call for routing decision\n            routing_decision = {\n                \"selected_agent\": \"code\",\n                \"confidence\": 0.92,\n                \"reasoning\": \"The query is asking about implementing a specific algorithm in code, which falls directly under the code agent's capabilities.\",\n                \"fallback_agent\": \"math\"\n            }\n            \n            # Step 2: If confidence is too low, request clarification\n            if routing_decision[\"confidence\"] < 0.7:\n                clarification_prompt = f\"\"\"I'm not entirely sure what you're asking about. To better assist you, could you please clarify:\n\nQuery: {query}\n\nI'm trying to determine if you need help with:\n1. {specialized_agents[routing_decision['selected_agent']]['description']}\n2. {specialized_agents[routing_decision['fallback_agent']]['description']}\n3. Something else entirely?\n\nPlease provide more details about what you're looking for.\n\"\"\"\n                # Simulate clarification request and response\n                clarification_response = \"I need help implementing a sorting algorithm in Python\"\n                \n                # Re-analyze with clarification\n                routing_prompt_with_clarification = f\"\"\"Reanalyze with this clarification:\n\nOriginal Query: {query}\n\nClarification: {clarification_response}\n\nAvailable agents:\n{json.dumps(specialized_agents, indent=2)}\n\nReturn a JSON object with the same fields as before.\n\"\"\"\n                # Simulate updated routing decision\n                routing_decision = {\n                    \"selected_agent\": \"code\",\n                    \"confidence\": 0.98,\n                    \"reasoning\": \"The clarification confirms this is a coding question about implementing a sorting algorithm in Python.\",\n                    \"fallback_agent\": \"math\"\n                }\n            \n            # Step 3: Route to the selected agent\n            selected_agent = routing_decision[\"selected_agent\"]\n            \n            # Step 4: Execute the appropriate agent\n            # In a real implementation, this would call the actual agent\n            # Here we'll simulate the selected agent's response\n            \n            agent_responses = {\n                \"math\": \"The mathematical solution to your problem involves using the quadratic formula...\",\n                \"code\": \"\"\"Here's a Python implementation of a merge sort algorithm:\n\n```python\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n        \n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n    \n    return merge(left, right)\n    \ndef merge(left, right):\n    result = []\n    i = j = 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    return result\n```\n\nThis implementation has O(n log n) time complexity and is stable but not in-place.\"\"\",\n                \"research\": \"Based on my research, here are the key facts about your topic...\",\n                \"creative\": \"Here's a creative story based on your prompt...\"\n            }\n            \n            agent_response = agent_responses[selected_agent]\n            \n            return {\n                \"status\": \"success\",\n                \"routing\": routing_decision,\n                \"response\": agent_response,\n                \"agent\": selected_agent\n            }\n            \n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\",

  'evaluator-optimizer': "# Evaluator-Optimizer pattern implementation\nimport openai\nimport json\nfrom typing import Dict, List, Any, Optional, Union\n\nclass EvaluatorOptimizerAgent:\n    def __init__(self, client, model: str = \"gpt-4\"):\n        self.client = client\n        self.model = model\n        \n    async def execute(self, query: str, initial_response: str) -> Dict[str, Any]:\n        \"\"\"Evaluate and optimize an initial response through multiple passes.\"\"\"\n        try:\n            evaluations = []\n            responses = [initial_response]\n            iterations = 0\n            max_iterations = 3\n            quality_threshold = 0.85\n            overall_score = 0\n            \n            evaluation_criteria = {\n                \"accuracy\": \"Is the information factually correct and reliable?\",\n                \"completeness\": \"Does the response fully address all aspects of the query?\",\n                \"clarity\": \"Is the response easy to understand and well-structured?\",\n                \"helpfulness\": \"Does the response provide practical, actionable information?\",\n                \"conciseness\": \"Is the response appropriately detailed without unnecessary content?\"\n            }\n            \n            while iterations < max_iterations:\n                iterations += 1\n                current_response = responses[-1]\n                \n                # Step 1: Evaluate the current response\n                evaluation_prompt = f\"\"\"Evaluate the following response to the query based on these criteria:\n\nQuery: {query}\n\nResponse to evaluate:\n{current_response}\n\nCriteria:\n{json.dumps(evaluation_criteria, indent=2)}\n\nFor each criterion, provide:\n1. A score from 0-10\n2. Specific feedback explaining the score\n3. Suggestions for improvement\n\nFinally, calculate an overall quality score as an average of all criteria (0-1 scale).\n\"\"\"\n                # Simulate LLM evaluation\n                if iterations == 1:\n                    evaluation_result = {\n                        \"criteria\": {\n                            \"accuracy\": {\"score\": 8, \"feedback\": \"Generally accurate but missing some key details about edge cases.\"},\n                            \"completeness\": {\"score\": 6, \"feedback\": \"Addresses main points but lacks depth on important aspects.\"},\n                            \"clarity\": {\"score\": 7, \"feedback\": \"Well-written but structure could be improved with headers.\"},\n                            \"helpfulness\": {\"score\": 7, \"feedback\": \"Provides useful information but lacks actionable examples.\"},\n                            \"conciseness\": {\"score\": 8, \"feedback\": \"Appropriately detailed with minimal fluff.\"}\n                        },\n                        \"overall_score\": 0.72,\n                        \"primary_improvements_needed\": [\n                            \"Add discussion of edge cases\",\n                            \"Include practical examples\",\n                            \"Improve structure with headers\"\n                        ]\n                    }\n                elif iterations == 2:\n                    evaluation_result = {\n                        \"criteria\": {\n                            \"accuracy\": {\"score\": 9, \"feedback\": \"Excellent accuracy with edge cases now addressed.\"},\n                            \"completeness\": {\"score\": 8, \"feedback\": \"Much more comprehensive with good depth.\"},\n                            \"clarity\": {\"score\": 9, \"feedback\": \"Clear structure with helpful headers.\"},\n                            \"helpfulness\": {\"score\": 8, \"feedback\": \"Good examples but could include more code samples.\"},\n                            \"conciseness\": {\"score\": 8, \"feedback\": \"Well-balanced detail level.\"}\n                        },\n                        \"overall_score\": 0.84,\n                        \"primary_improvements_needed\": [\n                            \"Add more code examples\",\n                            \"Stronger conclusion with next steps\"\n                        ]\n                    }\n                else:\n                    evaluation_result = {\n                        \"criteria\": {\n                            \"accuracy\": {\"score\": 9, \"feedback\": \"Excellent accuracy with comprehensive coverage.\"},\n                            \"completeness\": {\"score\": 9, \"feedback\": \"Thoroughly addresses all aspects of the query.\"},\n                            \"clarity\": {\"score\": 9, \"feedback\": \"Very clear structure and presentation.\"},\n                            \"helpfulness\": {\"score\": 10, \"feedback\": \"Extremely helpful with excellent examples and code samples.\"},\n                            \"conciseness\": {\"score\": 9, \"feedback\": \"Perfect balance of detail and brevity.\"}\n                        },\n                        \"overall_score\": 0.92,\n                        \"primary_improvements_needed\": []\n                    }\n                \n                evaluations.append(evaluation_result)\n                overall_score = evaluation_result[\"overall_score\"]\n                \n                # If quality is sufficient, break the loop\n                if overall_score > 0.85:\n                    // Quality threshold met, break the loop\n                    break\n                \n                # Step 2: Optimize the response based on evaluation\n                optimization_prompt = f\"\"\"Improve the following response based on this evaluation:\n\nQuery: {query}\n\nCurrent response:\n{current_response}\n\nEvaluation:\n{json.dumps(evaluation_result, indent=2)}\n\nCreate an improved response that addresses the identified issues while maintaining the strengths.\n\"\"\"\n                # Simulate LLM optimization\n                if iterations == 1:\n                    optimized_response = \"\"\"# Understanding Agent Patterns in Azure AI\n\n## Definition\nAgent patterns are reusable architectural approaches for building AI agents with specific capabilities and behaviors. In Azure AI, these patterns help developers create more effective and specialized agents.\n\n## Key Patterns\n1. **ReAct Pattern**: Combines reasoning and acting in alternating steps\n   - Example: An agent that analyzes a problem, takes an action, observes results, and iterates\n   - Edge cases: Handles situations where actions don't produce expected outcomes\n\n2. **Reflexion Pattern**: Enables self-critique to improve responses\n   - Process: Generate response → Evaluate → Refine → Repeat\n   - Edge cases: Includes mechanisms to break infinite improvement loops\n\n3. **Plan-Execute Pattern**: Creates a plan before executing actions\n   - Useful for: Complex multi-step tasks requiring coordination\n\n## Implementation in Azure AI\nAzure provides several services to implement these patterns:\n- Azure OpenAI Service\n- Azure AI SDK\n- Azure Cognitive Services\n\n## Practical Examples\nWhen building a research agent in Azure, you might use the ReAct pattern to:\n1. Parse a research question\n2. Search for relevant papers (action)\n3. Analyze results (reasoning)\n4. Refine the search (action)\n5. Synthesize findings (reasoning)\n\"\"\"\n                else:\n                    optimized_response = \"\"\"# Understanding Agent Patterns in Azure AI\n\n## Definition\nAgent patterns are reusable architectural approaches for building AI agents with specific capabilities and behaviors. In Azure AI, these patterns help developers create more effective and specialized agents.\n\n## Key Patterns\n1. **ReAct Pattern**: Combines reasoning and acting in alternating steps\n   - Example: An agent that analyzes a problem, takes an action, observes results, and iterates\n   - Edge cases: Handles situations where actions don't produce expected outcomes\n   ```python\n   async def react_pattern(query):\n       context = []\n       for step in range(max_steps):\n           # Reason about the current state\n           reasoning = await llm.complete(f\"Reason about: {query}, context: {context}\")\n           # Determine action\n           action = determine_next_action(reasoning)\n           # Execute action and observe\n           observation = await execute_action(action)\n           context.append(observation)\n   ```\n\n2. **Reflexion Pattern**: Enables self-critique to improve responses\n   - Process: Generate response → Evaluate → Refine → Repeat\n   - Edge cases: Includes mechanisms to break infinite improvement loops\n   ```python\n   async def reflexion_pattern(query):\n       response = await llm.complete(query)\n       for iteration in range(max_iterations):\n           evaluation = await llm.complete(f\"Evaluate: {response}\")\n           if evaluation.score > threshold:\n               break\n           response = await llm.complete(f\"Improve based on: {evaluation}\")\n   ```\n\n3. **Plan-Execute Pattern**: Creates a plan before executing actions\n   - Useful for: Complex multi-step tasks requiring coordination\n   ```python\n   async def plan_execute_pattern(query):\n       plan = await llm.complete(f\"Create a plan for: {query}\")\n       steps = parse_steps(plan)\n       results = []\n       for step in steps:\n           result = await execute_step(step)\n           results.append(result)\n   ```\n\n## Implementation in Azure AI\nAzure provides several services to implement these patterns:\n- Azure OpenAI Service: Provides the foundation models\n- Azure AI SDK: Offers tools for agent development\n- Azure Cognitive Services: Extends agent capabilities\n\n## Practical Examples\nWhen building a research agent in Azure, you might use the ReAct pattern to:\n1. Parse a research question\n2. Search for relevant papers (action)\n3. Analyze results (reasoning)\n4. Refine the search (action)\n5. Synthesize findings (reasoning)\n\n## Next Steps\nTo implement these patterns in your project:\n1. Identify which pattern best suits your use case\n2. Set up the appropriate Azure services\n3. Use the Azure AI SDK to implement the pattern\n4. Test with various inputs to ensure robust performance\n\nFor more advanced scenarios, consider combining multiple patterns or creating hybrid approaches tailored to your specific requirements.\n\"\"\"\n                \n                responses.append(optimized_response)\n            \n            return {\n                \"status\": \"success\",\n                \"iterations\": iterations,\n                \"evaluations\": evaluations,\n                \"responses\": responses,\n                \"final_response\": responses[-1],\n                \"final_score\": overall_score\n            }\n            \n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\",

  'plan-execute': "# Plan Execute pattern implementation\nimport openai\nimport json\nfrom typing import Dict, List, Any, Optional, Union\n\nclass PlanExecuteAgent:\n    def __init__(self, client, model: str = \"gpt-4\"):\n        self.client = client\n        self.model = model\n        \n    async def execute(self, query: str) -> Dict[str, Any]:\n        \"\"\"Execute the Plan-Execute agent pattern to solve complex problems.\"\"\"\n        try:\n            # Available tools\n            tools = {\n                \"search\": lambda query: {\n                    \"results\": f\"Search results for '{query}'\",\n                    \"sources\": [\"https://example.com/1\", \"https://example.com/2\"]\n                },\n                \"calculator\": lambda expr: {\"result\": eval(expr)},\n                \"weather\": lambda location: {\n                    \"temperature\": 72,\n                    \"condition\": \"sunny\",\n                    \"location\": location\n                },\n                \"translator\": lambda text_and_lang: {\n                    \"translation\": f\"Translated '{text_and_lang['text']}' to {text_and_lang['target_lang']}\"\n                }\n            }\n            \n            # Step 1: Planning phase - create detailed plan\n            planning_prompt = f\"\"\"You are a planning agent. Create a detailed step-by-step plan to solve this problem:\n\nProblem: {query}\n\nAvailable tools:\n- search: Search for information online\n- calculator: Perform mathematical calculations\n- weather: Get weather information for a location\n- translator: Translate text to another language\n\nYour plan should include:\n1. A breakdown of the problem into sequential steps\n2. Which tools to use for each step\n3. What information to pass between steps\n4. Success criteria for each step\n\nFormat your response as a JSON object with steps as an array.\n\"\"\"\n            # Simulate LLM call for planning\n            plan = {\n                \"goal\": \"Create a budget-friendly one-week travel itinerary for Paris\",\n                \"steps\": [\n                    {\n                        \"step_id\": 1,\n                        \"description\": \"Search for budget accommodation options in Paris\",\n                        \"tool\": \"search\",\n                        \"input\": \"budget hostels hotels Paris city center\",\n                        \"success_criteria\": \"At least 3 accommodation options with prices\"\n                    },\n                    {\n                        \"step_id\": 2,\n                        \"description\": \"Check weather forecast for Paris next week\",\n                        \"tool\": \"weather\",\n                        \"input\": \"Paris, France\",\n                        \"success_criteria\": \"Get temperature and conditions\"\n                    },\n                    {\n                        \"step_id\": 3,\n                        \"description\": \"Search for free or low-cost attractions in Paris\",\n                        \"tool\": \"search\",\n                        \"input\": \"free attractions museums Paris\",\n                        \"success_criteria\": \"List of at least 5 attractions with details\"\n                    },\n                    {\n                        \"step_id\": 4,\n                        \"description\": \"Calculate daily budget for food and transportation\",\n                        \"tool\": \"calculator\",\n                        \"input\": \"(50*7) + (20*7)\",\n                        \"success_criteria\": \"Total budget for food and transport\"\n                    },\n                    {\n                        \"step_id\": 5,\n                        \"description\": \"Learn basic French phrases for travelers\",\n                        \"tool\": \"translator\",\n                        \"input\": {\"text\": \"Hello, thank you, please, where is the museum?\", \"target_lang\": \"French\"},\n                        \"success_criteria\": \"Basic conversational phrases in French\"\n                    }\n                ]\n            }\n            \n            # Step 2: Execution phase - execute each step\n            results = []\n            \n            for step in plan[\"steps\"]:\n                # Get the appropriate tool\n                tool_name = step[\"tool\"]\n                tool_input = step[\"input\"]\n                \n                if tool_name in tools:\n                    try:\n                        # Execute the tool\n                        result = tools[tool_name](tool_input)\n                        \n                        step_result = {\n                            \"step_id\": step[\"step_id\"],\n                            \"description\": step[\"description\"],\n                            \"status\": \"success\",\n                            \"result\": result\n                        }\n                    except Exception as e:\n                        step_result = {\n                            \"step_id\": step[\"step_id\"],\n                            \"description\": step[\"description\"],\n                            \"status\": \"failed\",\n                            \"error\": str(e)\n                        }\n                else:\n                    step_result = {\n                        \"step_id\": step[\"step_id\"],\n                        \"description\": step[\"description\"],\n                        \"status\": \"failed\",\n                        \"error\": f\"Tool '{tool_name}' not found\"\n                    }\n                \n                results.append(step_result)\n            \n            # Step 3: Synthesis phase - combine results into final response\n            synthesis_prompt = f\"\"\"Synthesize the results of the executed plan into a coherent response:\n\nOriginal query: {query}\n\nPlan: {json.dumps(plan, indent=2)}\n\nResults: {json.dumps(results, indent=2)}\n\nCreate a comprehensive response that addresses the original query using these results.\n\"\"\"\n            # Simulate LLM call for synthesis\n            synthesis = \"\"\"# Budget-Friendly Paris Itinerary\n\n## Accommodation\nBased on our search, here are three budget-friendly options:\n- Le Village Hostel: €25/night in Montmartre\n- Generator Paris: €30/night near Canal St-Martin\n- Ibis Budget Paris La Villette: €55/night\n\n## Weather Forecast\nParis will be mostly sunny with temperatures around 72°F (22°C), perfect for sightseeing!\n\n## Free/Low-Cost Attractions\n1. **Notre-Dame Cathedral** (exterior only due to renovation)\n2. **Sacré-Cœur Basilica** - Free entry with panoramic city views\n3. **Luxembourg Gardens** - Beautiful public park\n4. **Père Lachaise Cemetery** - Historic resting place of famous figures\n5. **Centre Pompidou** - Free on first Sunday of month\n6. **Free museums on first Sunday of each month**\n\n## Budget Calculation\nWeekly budget for essentials:\n- Food: €50 × 7 days = €350\n- Transport: €20 × 7 days = €140\n- Total: €490 for food and transportation\n\n## Useful French Phrases\n- \"Bonjour\" - Hello\n- \"Merci\" - Thank you\n- \"S'il vous plaît\" - Please\n- \"Où est le musée?\" - Where is the museum?\n\n## Daily Itinerary Suggestion\n- **Day 1**: Explore Montmartre and Sacré-Cœur\n- **Day 2**: Visit the Louvre (free on Friday evenings for under-26s)\n- **Day 3**: Walk along Seine and visit Notre-Dame exterior\n- **Day 4**: Explore the Latin Quarter and Luxembourg Gardens\n- **Day 5**: Visit Centre Pompidou and Le Marais district\n- **Day 6**: Day trip to Versailles gardens (grounds are free)\n- **Day 7**: Explore local markets and Père Lachaise Cemetery\n\nThis itinerary gives you a mix of iconic landmarks and local experiences while keeping costs low.\n\"\"\"\n            \n            return {\n                \"status\": \"success\",\n                \"plan\": plan,\n                \"execution_results\": results,\n                \"final_response\": synthesis\n            }\n            \n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\",
};